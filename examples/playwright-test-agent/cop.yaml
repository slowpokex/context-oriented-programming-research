# Playwright Test Agent - COP Package
# Minimal manifest format - most settings auto-detected

meta:
  name: "playwright-test-agent"
  version: "1.0.0"
  description: "AI agent specialized in generating Playwright end-to-end tests"
  author: "COP Examples"
  license: "MIT"
  keywords: [testing, playwright, e2e, automation]

# Core Context Definition
context:
  system:
    source: "./prompts/system.md"
    variables:
      project_name:
        type: string
        default: "My Web App"
        description: "Name of the project being tested"
      project_type:
        type: string
        default: "web-application"
        enum: [web-application, spa, ssr, static-site, e-commerce, dashboard]
      language:
        type: string
        default: "typescript"
        enum: [typescript, javascript]
      base_url:
        type: string
        default: "http://localhost:3000"
        format: url
      test_directory:
        type: string
        default: "tests/e2e"

  personas:
    default: "technical"
    available:
      technical:
        source: "./personas/technical.yaml"
        description: "Precise, code-focused communication"
      helpful:
        source: "./personas/helpful.yaml"
        description: "Friendly, educational approach"
      concise:
        source: "./personas/concise.yaml"
        description: "Brief, direct responses"

  knowledge:
    - name: "playwright_best_practices"
      source: "./knowledge/best-practices.md"
      type: static
    - name: "common_patterns"
      source: "./knowledge/common-patterns.md"
      type: static
    - name: "selectors_guide"
      source: "./knowledge/selectors-guide.md"
      type: static

  guardrails:
    - name: "safety"
      source: "./guardrails/safety.yaml"
      priority: 100
    - name: "code_quality"
      source: "./guardrails/code-quality.yaml"
      priority: 90
    - name: "security"
      source: "./guardrails/security.yaml"
      priority: 95

  tools:
    - name: "read_file"
      source: "./tools/read_file.yaml"
    - name: "analyze_project_structure"
      source: "./tools/analyze_project_structure.yaml"
    - name: "search_codebase"
      source: "./tools/search_codebase.yaml"
    - name: "check_existing_tests"
      source: "./tools/check_existing_tests.yaml"
    - name: "validate_test_code"
      source: "./tools/validate_test_code.yaml"

# Build Configuration - minimal, uses defaults
build:
  # Local LLM configuration for synthetic data generation
  local_llm:
    endpoint: "http://localhost:1234/v1"
    model: "qwen/qwen3-4b-thinking-2507"  # Update to your loaded model
  
  # Linking enabled - RAG auto-enabled for synthetic generation
  linking:
    enabled: true

  # Synthetic data generation
  synthetic:
    enabled: true
    samples: 15
    scenarios:
      - "generate test for login page"
      - "write e2e test for navigation menu"
      - "create test for form validation"
      - "test user authentication flow"
      - "generate test for shopping cart"
      - "write test for search functionality"
      - "create accessibility test"
      - "test responsive design"
      - "generate API mocking test"
      - "write test for error handling"

# MCP Integration - External tools for the agent
mcp:
  servers:
    # Filesystem access (read/write files)
    - name: filesystem
      command: npx
      args: ["-y", "@modelcontextprotocol/server-filesystem", "./"]
      enabled: true
  
  # Instructions added to system prompt when MCP tools are available
  system_instructions: |
    You have access to filesystem tools that allow you to:
    - Read files from the project
    - Write new files (tests, code, etc.)
    - List directories
    - Create directories
    
    When asked to create or write tests, USE the filesystem tools to actually 
    write the files. Don't just show the code - save it to the appropriate location.
#     
#     # Browser automation
#     - name: browser
#       command: npx
#       args: ["-y", "@anthropic/mcp-browser"]
#       enabled: false
#     
#     # Custom MCP server
#     - name: my-server
#       command: python
#       args: ["-m", "my_mcp_server"]
#       env:
#         API_KEY: "${MY_API_KEY}"
#       enabled: false

# Test Suites
evaluation:
  test_suites:
    - name: "behavioral"
      path: "./tests/behavioral/"
      type: llm-judged
    - name: "safety"
      path: "./tests/safety/"
      type: adversarial
    - name: "code_quality"
      path: "./tests/code-quality/"
      type: deterministic
